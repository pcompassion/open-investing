#+title: Frozen Lake
#+PROPERTY: header-args:jupyter-python :tangle frozen-lake.py

#+begin_src jupyter-python
import numpy as np

# Define the environment
n_states = 16  # For the 4x4 grid
n_actions = 4  # Up, Down, Left, Right

# Initialize the Q-table with zeros
Q = np.zeros((n_states, n_actions))

#+end_src

#+RESULTS:
#+begin_example
[[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]]
#+end_example

#+begin_src jupyter-python
frozen_lake_list = [
    'SFFF',
    'FHFH',
    'FFFH',
    'HFFG'
]

#+end_src

#+RESULTS:



#+begin_src jupyter-python
frozen_lake = np.array([list(row) for row in frozen_lake_list])
#+end_src

#+RESULTS:

#+begin_src jupyter-python
def state_to_coord(state):
    """Convert a state number to a coordinate (i, j) on the grid."""
    return divmod(state, 4)

def coord_to_state(coord):
    """Convert a grid coordinate (i, j) to a state number."""
    i, j = coord
    return i * 4 + j


def action_str(action):
    res = None
    if action == 0:  # UP
        res = 'up'
    elif action == 1:  # DOWN
        res = 'down'
    elif action == 2:  # LEFT
        res = 'left'
    elif action == 3:  # RIGHT
        res = 'right'

    return res

def next_state(state, action):
    """Determine the next state given an action."""
    i, j = state_to_coord(state)
    if action == 0:  # UP
        i = max(i - 1, 0)
    elif action == 1:  # DOWN
        i = min(i + 1, 3)
    elif action == 2:  # LEFT
        j = max(j - 1, 0)
    elif action == 3:  # RIGHT
        j = min(j + 1, 3)
    return coord_to_state((i, j))

def get_reward(state):
    i, j = state_to_coord(state)
    cell = frozen_lake[i][j]
    if cell == 'G':
        return 1
    elif cell == 'H':
        return 0
    else:
        return -0.01

def choose_action(state, epsilon):
    if np.random.uniform(0, 1) < epsilon:
        return np.random.choice(n_actions)  # Explore: choose a random action
    else:
        return np.argmax(Q[state, :])  # Exploit: choose the action with max Q-value


#+end_src

#+RESULTS:


#+begin_src jupyter-python :tangle no
for i in range(16):
    print(divmod(i, 4))
#+end_src

#+RESULTS:
#+begin_example
(0, 0)
(0, 1)
(0, 2)
(0, 3)
(1, 0)
(1, 1)
(1, 2)
(1, 3)
(2, 0)
(2, 1)
(2, 2)
(2, 3)
(3, 0)
(3, 1)
(3, 2)
(3, 3)
#+end_example

#+begin_src jupyter-python
# Hyperparameters
alpha = 0.8  # learning rate
gamma = 0.95  # discount factor
epsilon = 0.1  # exploration rate
total_episodes = 10000  # number of episodes to train on

# Q-learning Loop
def train(total_episodes):
    for episode in range(total_episodes):
        state = 0  # start at the beginning of the lake
        while True:  # Continue until a terminal state (H or G) is reached
            # Choose action using epsilon-greedy strategy
            action = choose_action(state, epsilon)

            # Take action and observe new state and reward
            new_state = next_state(state, action)
            reward = get_reward(new_state)

            # Update Q-value using the Q-learning update rule
            Q[state, action] = (1 - alpha) * Q[state, action] + \
                               alpha * (reward + gamma * np.max(Q[new_state, :]))

            # Move to the next state
            state = new_state

            # Check for terminal states
            cell_content = frozen_lake[state_to_coord(state)]

            if cell_content == 'H':
                Q[new_state, :] = -1  # or any other distinct negative value
            if cell_content in ['G', 'H']:
                break

    print("Training complete!")

train(total_episodes)
#+end_src

#+RESULTS:
: Training complete!

#+begin_src jupyter-python :tangle no
frozen_lake
#+end_src

#+RESULTS:
: array([['S', 'F', 'F', 'F'],
:        ['F', 'H', 'F', 'H'],
:        ['F', 'F', 'F', 'H'],
:        ['H', 'F', 'F', 'G']], dtype='<U1')

#+begin_src jupyter-python :tangle no
Q
#+end_src

#+RESULTS:
#+begin_example
array([[ 0.68211027,  0.72853712,  0.68211027,  0.72853712],
       [ 0.72853712, -0.95      ,  0.68211027,  0.7774075 ],
       [ 0.7774075 ,  0.82885   ,  0.72853712,  0.72853712],
       [-0.03692546, -0.912     ,  0.7774075 , -0.03692546],
       [ 0.68211027,  0.7774075 ,  0.72853712, -0.95      ],
       [-1.        , -1.        , -1.        , -1.        ],
       [ 0.7774075 ,  0.883     , -0.95      , -0.95      ],
       [-1.        , -1.        , -1.        , -1.        ],
       [ 0.72853712, -0.95      ,  0.7774075 ,  0.82885   ],
       [-0.95      ,  0.883     ,  0.7774075 ,  0.883     ],
       [ 0.82885   ,  0.94      ,  0.82885   , -0.95      ],
       [-1.        , -1.        , -1.        , -1.        ],
       [-1.        , -1.        , -1.        , -1.        ],
       [ 0.82885   ,  0.883     , -0.95      ,  0.94      ],
       [ 0.883     ,  0.94      ,  0.883     ,  1.        ],
       [ 0.        ,  0.        ,  0.        ,  0.        ]])
#+end_example


#+begin_src jupyter-python
policy = np.argmax(Q, axis=1)
#+end_src

#+RESULTS:
: array([1, 3, 1, 2, 1, 0, 1, 0, 3, 1, 1, 0, 0, 3, 3, 0])


#+begin_src jupyter-python
def test_policy(policy, num_episodes=1000):
    success_count = 0

    for episode in range(num_episodes):
        state = 0  # start at the beginning of the lake
        step = 0
        while True:  # Continue until a terminal state (H or G) is reached
            action = policy[state]  # choose action from policy
            state = next_state(state, action)
            step += 1
            coord = state_to_coord(state)
            print(f'action: {action_str(action)}, coord: {coord}')

            # Check for terminal states
            cell_content = frozen_lake[coord]
            if cell_content == 'G':
                success_count += 1  # Increment success count if goal reached
                print(f"Goal reached in {step}")
                break
            elif cell_content == 'H':
                break  # end episode if fallen into a hole

    success_rate = success_count / num_episodes
    return success_rate


num_episodes = 10
success_rate = test_policy(policy, num_episodes)
print(f"Policy success rate over {num_episodes} episodes: {success_rate * 100:.2f}%")

#+end_src

#+RESULTS:
#+begin_example
action: down, coord: (1, 0)
action: down, coord: (2, 0)
action: right, coord: (2, 1)
action: down, coord: (3, 1)
action: right, coord: (3, 2)
action: right, coord: (3, 3)
Goal reached in 6
action: down, coord: (1, 0)
action: down, coord: (2, 0)
action: right, coord: (2, 1)
action: down, coord: (3, 1)
action: right, coord: (3, 2)
action: right, coord: (3, 3)
Goal reached in 6
action: down, coord: (1, 0)
action: down, coord: (2, 0)
action: right, coord: (2, 1)
action: down, coord: (3, 1)
action: right, coord: (3, 2)
action: right, coord: (3, 3)
Goal reached in 6
action: down, coord: (1, 0)
action: down, coord: (2, 0)
action: right, coord: (2, 1)
action: down, coord: (3, 1)
action: right, coord: (3, 2)
action: right, coord: (3, 3)
Goal reached in 6
action: down, coord: (1, 0)
action: down, coord: (2, 0)
action: right, coord: (2, 1)
action: down, coord: (3, 1)
action: right, coord: (3, 2)
action: right, coord: (3, 3)
Goal reached in 6
action: down, coord: (1, 0)
action: down, coord: (2, 0)
action: right, coord: (2, 1)
action: down, coord: (3, 1)
action: right, coord: (3, 2)
action: right, coord: (3, 3)
Goal reached in 6
action: down, coord: (1, 0)
action: down, coord: (2, 0)
action: right, coord: (2, 1)
action: down, coord: (3, 1)
action: right, coord: (3, 2)
action: right, coord: (3, 3)
Goal reached in 6
action: down, coord: (1, 0)
action: down, coord: (2, 0)
action: right, coord: (2, 1)
action: down, coord: (3, 1)
action: right, coord: (3, 2)
action: right, coord: (3, 3)
Goal reached in 6
action: down, coord: (1, 0)
action: down, coord: (2, 0)
action: right, coord: (2, 1)
action: down, coord: (3, 1)
action: right, coord: (3, 2)
action: right, coord: (3, 3)
Goal reached in 6
action: down, coord: (1, 0)
action: down, coord: (2, 0)
action: right, coord: (2, 1)
action: down, coord: (3, 1)
action: right, coord: (3, 2)
action: right, coord: (3, 3)
Goal reached in 6
Policy success rate over 10 episodes: 100.00%
#+end_example
